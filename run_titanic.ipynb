{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mymodels.data_engineer import data_engineer\n",
    "from mymodels.pipeline import MyPipeline\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Global settings for font\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# For debugging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Construct an object for workflow\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **results_dir**: Directory path where your results will be stored. Accepts either a string or pathlib.Path object. The directory will be created if it doesn't exist.\n",
    "\n",
    "- **random_state**: Random seed for the entire pipeline (data splitting, model tuning, etc.). (Default is 0)\n",
    "\n",
    "- **show**: Whether to display the figure on the screen. (Default is `False`)\n",
    "\n",
    "- **plot_format**: Output format for figures. (Default is jpg)\n",
    "\n",
    "- **plot_dpi**: Controlling the resolution of output figures. (Default is 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyPipeline(\n",
    "    results_dir = \"results/titanic\",\n",
    "    random_state = 0,\n",
    "    show = False,\n",
    "    plot_format = \"jpg\",\n",
    "    plot_dpi = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **file_path**: In which the data you want to input. **.csv format is mandatory**. \n",
    "\n",
    "- **y**: The target you want to predict. A `str` object represented column name or a `int` object represented the column index are both acceptable.\n",
    "\n",
    "- **x_list**: A `list` object (or a `tuple` object) of the independent variables. Each element in `list` (or `tuple`) must be a `str` object represented column name or a `int` object represented the column index.\n",
    "\n",
    "- **index_col**: An `int` object or `str` object representing the index column. (Default is `None`)\n",
    "\n",
    "    > It's STRONGLY RECOMMENDED to set the index column if you want to output the raw data and the shap values. Also, it's acceptable to provide a `list` object (or a `tuple` object) for representing multiple index columns. \n",
    "\n",
    "- **test_ratio**: The proportion of test data. (Default is 0.3)\n",
    "\n",
    "- **inspect**: Whether to display the y column or the independent variables you chose in the terminal. (Default is `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.load(\n",
    "    file_path = \"data/titanic.csv\",\n",
    "    y = \"Survived\",\n",
    "    x_list = [\"Pclass\", \"Sex\", \"Embarked\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"],\n",
    "    index_col = [\"PassengerId\"],\n",
    "    test_ratio = 0.3,\n",
    "    inspect = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose the data\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `sample_k`: set the sampling ratio for diagnostic data.\n",
    "\n",
    "The `mymodels.diagnose()` method provides visual data diagnostics, including:\n",
    "\n",
    "- Data types\n",
    "\n",
    "- Missing data counts\n",
    "\n",
    "- Distribution, count, and proportion of categorical variables (displayed as bar charts)\n",
    "\n",
    "- Distribution of continuous variables (displayed as violin plots and box plots)\n",
    "\n",
    "- Correlation of continuous variables, **strongly recommended to review before SHAP analysis** (displayed as heatmaps using Spearman and Pearson correlation tests)\n",
    "\n",
    "It's strongly recommanded to run step-by-step. Run the `mymodels.diagnose()` for data diagnosis firstly, then setting feature engineering parameters or customizing feature engineering based on the results.\n",
    "\n",
    "**Note: This method only diagnoses the training set after data splitting.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.diagnose(sample_k=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **outlier_col**: Specify the columns with outliers. (Default is `None`)\n",
    "\n",
    "  > It's not supported currently, but will be added in the future.\n",
    "\n",
    "- **missing_values_cols**: A `list` (or a `tuple`) object for representing the columns have missing values. (Default is `None`)\n",
    "\n",
    "- **impute_method**: A `str`, `list` (or a `tuple`) object for representing the impute methods. (Default is `None`)\n",
    "\n",
    "  All impute methods in `sklearn.preprocessing.SimpleImputer` are supported. \n",
    "\n",
    "  If a `str` is presented, all columns in `missing_values_cols` will be implemented by the given method. If a `list` (or a `tuple`) is presented ,the length of this parameter must match `missing_values_cols`, and they must either both be provided or both be set to `None`.\n",
    "\n",
    "- **cat_features**: A `list` (or a `tuple`) object for representing the categorical columns. (Default is `None`)\n",
    "\n",
    "- **encode_method**: A `str` object representing the encode method, or a `list` (or a `tuple`) of encode methods are both acceptable. (Default is `None`)\n",
    "\n",
    "  If a `str` is presented, all columns in `cat_features` will be implemented by the given method. If a `list` (or a `tuple`) is presented ,the length of this parameter must match `cat_features`, and they must either both be provided or both be set to `None`.\n",
    "\n",
    "  > A full list of supported encode methods can be found at the end.\n",
    "\n",
    "- **scale_cols**: A `list` (or a `tuple`) object for representing the columns need scaling. (Default is `None`)\n",
    "\n",
    "- **scale_method**: A `str` object representing the scale method, or a `list` (or a `tuple`) of scale methods are both acceptable. (Default is `None`)\n",
    "\n",
    "  Currently the `sklearn.preprocessing.StandardScaler` and `sklearn.preprocessing.MinMaxScaler` are supported.\n",
    "\n",
    "  If a `str` is presented, all columns in `scale_cols` will be implemented by the given method. If a `list` (or a `tuple`) is presented ,the length of this parameter must match `scale_cols`, and they must either both be provided or both be set to `None`.\n",
    "\n",
    "- **n_jobs**: Parallel execution in data engineering. Speed up in excuting large dataset. (Default is `1`)\n",
    "\n",
    "- **verbose**: Whether to print the infomation in transformation. (Default is `False`)\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "The `mymodels.data_engineer()` method return a `sklearn.pipeline.Pipeline` object, which will be passed into the `mymodels.optimize()` method below. This `Pipeline` will preprocess the data before model training, including:\n",
    "\n",
    "- Being called in each fold of cross-validation to fit and transform the training set, then transform the validation set. Each fold will create an independent pipeline object that doesn't affect others.\n",
    "\n",
    "- After hyperparameter optimization, a new pipeline object will be created to fit and transform all training data, then transform the test set.\n",
    "\n",
    "Users can define their own pipeline objects for feature engineering (e.g., adding feature selection steps), as long as they conform to the `sklearn.pipeline.Pipeline` class. However, users must test these themselves, and this project takes no responsibility for any issues that arise.\n",
    "\n",
    "The pipeline will be exported to a `data_engineer_pipeline.joblib` file in the specified result path, which users can manually load and reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# User-defined pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "self_defined_data_engineer_pipeline = Pipeline()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Return an instance of `sklearn.pipeline.Pipeline` object\n",
    "# User can define their own pipeline\n",
    "data_engineer_pipeline = data_engineer(\n",
    "    outlier_cols = None,\n",
    "    missing_values_cols = [\"Age\", \"Embarked\"],\n",
    "    impute_method = [\"mean\", \"most_frequent\"],\n",
    "    cat_features = [\"Sex\", \"Embarked\"],\n",
    "    encode_method = [\"onehot\", \"onehot\"],\n",
    "    # scale_cols = [\"Fare\"],\n",
    "    # scale_method = [\"standard\"],\n",
    "    n_jobs = 5,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the optimization\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **model_name**: the model you want to use. In this example, `xgbc` represented XGBoost classifier, other model name like `catr` means CatBoost regressor. A full list of model names representing different models and tasks can be found at the end.\n",
    "\n",
    "- **data_engineer_pipeline**: A `sklearn.pipeline.Pipeline` object for data engineering.\n",
    "\n",
    "- **cv**: Cross-validation in the tuning process. (Default is 5)\n",
    "\n",
    "- **trials**: How many trials in the Bayesian tuning process (Based on [Optuna](https://optuna.org/)). 10 trials is just for demonstration, users should set it to a larger value for better hyperparameter optimization. (Default is 50)\n",
    "\n",
    "- **n_jobs**: How many cores will be used in the cross-validation process. It's recommended to use the same value as `cv`. (Default is 5)\n",
    "\n",
    "- **cat_features**: A `list` (or a `tuple`) of categorical features to specify for **CatBoost model ONLY**. A `list` (or a `tuple`) of `str` representing the column names. (Default is `None`)\n",
    "\n",
    "  > If the model_name is neither `catc` nor `catr` (which represent CatBoost models), this parameter must be set to `None`; otherwise, an assertion error will occur.\n",
    "\n",
    "- **optimize_history**: Whether to save the optimization history. (Default is `True`)\n",
    "\n",
    "- **save_optimal_params**: Whether to save the best parameters. (Default is `True`)\n",
    "\n",
    "- **save_optimal_model**: Whether to save the optimal model. (Default is `True`)\n",
    "\n",
    "### Output\n",
    "\n",
    "Several files will be output in the results directory:\n",
    "\n",
    "- `params.yml` will document the best parameters.\n",
    "\n",
    "- `mapping.json` will document the mapping relationship between the categorical features and the encoded features.\n",
    "\n",
    "- `optimal-model.joblib` will save the optimal model from sklearn.\n",
    "\n",
    "- `optimal-model.cbm` will save the optimal model from CatBoost.\n",
    "\n",
    "- `optimal-model.txt` will save the optimal model from LightGBM.\n",
    "\n",
    "- `optimal-model.json` will save the optimal model from XGBoost.\n",
    "\n",
    "- `optimal-model.pkl` will save all types of optimal model for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.optimize(\n",
    "    model_name = \"xgbc\",\n",
    "    data_engineer_pipeline = data_engineer_pipeline,\n",
    "    cv = 5,\n",
    "    trials = 10,\n",
    "    n_jobs = 5,\n",
    "    cat_features = None,  # For CatBoost ONLY\n",
    "    optimize_history = True,\n",
    "    save_optimal_params = True,\n",
    "    save_optimal_model = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model's accuracy\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **save_raw_data**: Whether to save the raw prediction data. Default is `True`.\n",
    "\n",
    "### Output\n",
    "\n",
    "The accuracy results will be output to the directory you defined above:\n",
    "\n",
    "- A `.yml` file named `accuracy` will document the results of model's accuracy.\n",
    "\n",
    "- A figure named `roc_curve_plot` document the classification accuracy.\n",
    "\n",
    "- Or a figure named `accuracy_plot` (it is a scatter plot) for regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.evaluate(save_raw_data = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the model using SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **select_background_data**: The data used for **background value calculation**. (Default is `\"train\"`)\n",
    "\n",
    "    Default is `\"train\"`, meaning that all data in the training set will be used. `\"test\"` means that all data in the test set will be used. `\"all\"` means that all data in the training and test set will be used. \n",
    "\n",
    "- **select_shap_data**: The data used for **calculating SHAP values**. Default is `\"test\"`, meaning that all data in the test set will be used. `\"all\"` means that all data in the training and test set will be used. (Default is `\"test\"`)\n",
    "\n",
    "- **sample_background_data_k**: Sampling the samples in the training set for **background value calculation**. (Default is `None`)\n",
    "    \n",
    "    Default `None`, meaning that all data in the training set will be used. An integer value means an actual number of data, while a float (i.e., 0.5) means the proportion in the training set for it. \n",
    "\n",
    "- **sample_shap_data_k**: Similar meaning to the `sample_background_data_k`. The test set will be implemented for **SHAP value calculation**. (Default is `None`)\n",
    "\n",
    "- **output_raw_data**: Whether to save the raw data. Default is `False`.\n",
    "\n",
    "> SHAP currently doesn't support multi-class classification tasks when using **GBDT** models. This limitation may affect the interpretability results and users should verify compatibility with their use case.\n",
    "\n",
    "### Output\n",
    "\n",
    "The figures (Summary plot, Dependence plots) will be output to the directory you defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.explain(\n",
    "    select_background_data = \"train\",\n",
    "    select_shap_data = \"test\",\n",
    "    sample_background_data_k = 50,\n",
    "    sample_shap_data_k = 50,\n",
    "    output_raw_data = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
