{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\mymodels\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from mymodels import data_engineer\n",
    "from mymodels import MyModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Construct an object for workflow\n",
    "\n",
    "- **random_state**: Random seed for the entire pipeline (data splitting, model tuning, etc.). (Default is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyModel(random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Engineering\n",
    "\n",
    "### Note\n",
    "\n",
    "The `mymodels.data_engineer()` method return a `sklearn.pipeline.Pipeline` object, which will be passed into the `mymodels.optimize()` method below. This `Pipeline` will preprocess the data before model training, including:\n",
    "\n",
    "- Being called in each fold of cross-validation to fit and transform the training set, then transform the validation set. Each fold will create an independent pipeline object that doesn't affect others.\n",
    "\n",
    "- After hyperparameter optimization, a new pipeline object will be created to fit and transform all training data, then transform the test set.\n",
    "\n",
    "Users can define their own pipeline objects for feature engineering (e.g., adding feature selection steps), as long as they conform to the `sklearn.pipeline.Pipeline` class. However, users must test these themselves, and this project takes no responsibility for any issues that arise.\n",
    "\n",
    "The pipeline will be exported to a `data_engineer_pipeline.joblib` file in the specified result path, which users can manually load and reuse.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **outlier_cols**: Specify the columns with outliers. (Default is `None`)\n",
    "\n",
    "  > It's not supported currently, but will be added in the future.\n",
    "\n",
    "- **missing_values_cols**: A `list` (or a `tuple`) object for representing the columns have missing values. (Default is `None`)\n",
    "\n",
    "- **impute_method**: A `str`, `list` (or a `tuple`) object for representing the impute methods. (Default is `None`)\n",
    "\n",
    "  All impute methods in `sklearn.preprocessing.SimpleImputer` are supported. \n",
    "\n",
    "  If a `str` is presented, all columns in `missing_values_cols` will be implemented by the given method. If a `list` (or a `tuple`) is presented ,the length of this parameter must match `missing_values_cols`, and they must either both be provided or both be set to `None`.\n",
    "\n",
    "- **cat_features**: A `list` (or a `tuple`) object for representing the categorical columns. (Default is `None`)\n",
    "\n",
    "- **encode_method**: A `str` object representing the encode method, or a `list` (or a `tuple`) of encode methods are both acceptable. (Default is `None`)\n",
    "\n",
    "  If a `str` is presented, all columns in `cat_features` will be implemented by the given method. If a `list` (or a `tuple`) is presented ,the length of this parameter must match `cat_features`, and they must either both be provided or both be set to `None`.\n",
    "\n",
    "  > A full list of supported encode methods can be found at the end.\n",
    "\n",
    "- **scale_cols**: A `list` (or a `tuple`) object for representing the columns need scaling. (Default is `None`)\n",
    "\n",
    "- **scale_method**: A `str` object representing the scale method, or a `list` (or a `tuple`) of scale methods are both acceptable. (Default is `None`)\n",
    "\n",
    "  Currently the `sklearn.preprocessing.StandardScaler` and `sklearn.preprocessing.MinMaxScaler` are supported.\n",
    "\n",
    "  If a `str` is presented, all columns in `scale_cols` will be implemented by the given method. If a `list` (or a `tuple`) is presented ,the length of this parameter must match `scale_cols`, and they must either both be provided or both be set to `None`.\n",
    "\n",
    "- **n_jobs**: Parallel execution in data engineering. Speed up in excuting large dataset. (Default is `1`)\n",
    "\n",
    "- **verbose**: Whether to print the infomation in transformation. (Default is `False`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# User-defined pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "self_defined_data_engineer_pipeline = Pipeline()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data_engineer_pipeline = data_engineer(\n",
    "    outlier_cols = None,\n",
    "    missing_values_cols = [\"Age\", \"Embarked\"],\n",
    "    impute_method = [\"mean\", \"most_frequent\"],\n",
    "    cat_features = [\"Sex\", \"Embarked\"],\n",
    "    encode_method = [\"onehot\", \"onehot\"],\n",
    "    # scale_cols = [\"Fare\"],\n",
    "    # scale_method = [\"standard\"],\n",
    "    n_jobs = 5,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data and configurations\n",
    "\n",
    "- **data**: Use pandas for data input. \n",
    "\n",
    "    > It's STRONGLY RECOMMENDED to set the index column if you want to output the raw data and the shap values. Also, it's acceptable to provide a `list` object (or a `tuple` object) for representing multiple index columns. \n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **model_name**: the model you want to use. In this example, `xgbc` represented XGBoost classifier, other model name like `catr` means CatBoost regressor. A full list of model names representing different models and tasks can be found at the end.\n",
    "\n",
    "- **input_data**: A `pd.Dataframe` of pandas for input. \n",
    "\n",
    "- **y**: The target you want to predict. A `str` object represented column name or a `int` object represented the column index are both acceptable.\n",
    "\n",
    "- **x_list**: A `list` object (or a `tuple` object) of the independent variables. Each element in `list` (or `tuple`) must be a `str` object represented column name or a `int` object represented the column index.\n",
    "\n",
    "- **test_ratio**: The proportion of test data. (Default is 0.3)\n",
    "\n",
    "- **stratify**: Whether or not to split the data in a stratified fashion. (Default is False)\n",
    "\n",
    "\n",
    "- **data_engineer_pipeline**: A `sklearn.pipeline.Pipeline` object for data engineering.\n",
    "\n",
    "- **cat_features**: A `list` (or a `tuple`) of categorical features to specify for **CatBoost model ONLY**. A `list` (or a `tuple`) of `str` representing the column names. (Default is `None`)\n",
    "\n",
    "  > If the model_name is neither `catc` nor `catr` (which represent CatBoost models), this parameter must be set to `None`; otherwise, an assertion error will occur.\n",
    "\n",
    "\n",
    "- **model_configs_path**: The hyperparameters tuning space can be found in `model_configs.yml` file, user can change the hyperparameters to fit their needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Model <class 'sklearn.ensemble._forest.RandomForestClassifier'> does not accept cat_features parameter. The provided cat_features value will be ignored.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/titanic.zip\", encoding=\"utf-8\",\n",
    "                   na_values=np.nan, index_col=[\"PassengerId\"])\n",
    "\n",
    "mymodel.load(\n",
    "    model_name = \"rfc\",\n",
    "    input_data = data,\n",
    "    y = \"Survived\",\n",
    "    x_list = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"],\n",
    "    test_ratio = 0.3,\n",
    "    stratify = False,\n",
    "    data_engineer_pipeline = data_engineer_pipeline,\n",
    "    cat_features = [\"Sex\", \"Embarked\"],\n",
    "    model_configs_path = \"model_configs.yml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Format the visualization and output\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **results_dir**: Directory path where your results will be stored. Accepts either a string or pathlib.Path object. The directory will be created if it doesn't exist.\n",
    "\n",
    "- **show**: Whether to display the figure on the screen. (Default is `False`)\n",
    "\n",
    "- **plot_format**: Output format for figures. (Default is jpg)\n",
    "\n",
    "- **plot_dpi**: Controlling the resolution of output figures. (Default is 500)\n",
    "\n",
    "- **save_optimal_model**: Whether to save the optimal model. (Default is `False`)\n",
    "\n",
    "    If `save_optimal_model` is `True`:\n",
    "\n",
    "    - `optimal-model.joblib` will save the optimal model from sklearn.\n",
    "\n",
    "    - `optimal-model.cbm` will save the optimal model from CatBoost.\n",
    "\n",
    "    - `optimal-model.txt` will save the optimal model from LightGBM.\n",
    "\n",
    "    - `optimal-model.json` will save the optimal model from XGBoost.\n",
    "\n",
    "    - `optimal-model.pkl` will save all types of optimal model for compatibility.\n",
    "\n",
    "- **save_raw_data**: Whether to save the raw data. (Default is `False`)\n",
    "\n",
    "- **save_shap_values**: Whether to export the shap values. (Default is `False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.format(\n",
    "    results_dir = \"results/titanic\",\n",
    "    show = False,\n",
    "    plot_format = \"jpg\",\n",
    "    plot_dpi = 500,\n",
    "    save_optimal_model = True,\n",
    "    save_raw_data = True,\n",
    "    save_shap_values = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diagnose the data\n",
    "\n",
    "The `mymodels.diagnose()` method provides visual data diagnostics, including:\n",
    "\n",
    "- Data types\n",
    "\n",
    "- Missing data counts\n",
    "\n",
    "- Distribution, count, and proportion of categorical variables (displayed as bar charts)\n",
    "\n",
    "- Distribution of continuous variables (displayed as violin plots and box plots)\n",
    "\n",
    "- Correlation of continuous variables, **strongly recommended to review before SHAP analysis** (displayed as heatmaps using Spearman and Pearson correlation tests)\n",
    "\n",
    "**Note: This method only diagnoses the training set after data splitting.**\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `sample_k`: set the sampling ratio for diagnostic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================\n",
      "Data diagnosis is performed on TRAINING DATASET ONLY.\n",
      "=========================================================\n",
      "\n",
      "\n",
      "Categorical Features Statistics:\n",
      "Feature Name  Count  Null Count Null Ratio  Unique Count Unique Ratio\n",
      "         Sex    623           0      0.00%             2        0.32%\n",
      "    Embarked    623           2      0.32%             3        0.48%\n",
      "\n",
      "Numerical Features Statistics:\n",
      "Feature Name  Count  Null Count Null Ratio  Min   25% Median   75%    Max  Mean   Std Kurtosis Skewness\n",
      "      Pclass    623           0      0.00% 1.00  1.50   3.00  3.00   3.00  2.29  0.84    -1.34    -0.58\n",
      "         Age    623         121     19.42% 0.67 21.00  29.00 38.00  80.00 29.92 14.51     0.25     0.34\n",
      "       SibSp    623           0      0.00% 0.00  0.00   0.00  1.00   8.00  0.53  1.16    19.25     3.91\n",
      "       Parch    623           0      0.00% 0.00  0.00   0.00  0.00   6.00  0.39  0.83     9.77     2.75\n",
      "        Fare    623           0      0.00% 0.00  7.92  15.00 31.39 512.33 32.46 48.26    35.27     4.84\n"
     ]
    }
   ],
   "source": [
    "mymodel.diagnose(sample_k=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizing\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **strategy**: The strategy for hyperparameters searching.\n",
    "  - `tpe`: `TPESampler` of Optuna.\n",
    "  - `random`: `RandomSampler` of Optuna.\n",
    "\n",
    "- **cv**: Cross-validation in the tuning process. (Default is 5)\n",
    "\n",
    "- **trials**: How many trials in the Bayesian tuning process (Based on [Optuna](https://optuna.org/)). 10 trials is just for demonstration, users should set it to a larger value for better hyperparameter optimization. (Default is 100)\n",
    "\n",
    "- **n_jobs**: How many cores will be used in the cross-validation process. It's recommended to use the same value as `cv`. (Default is 5)\n",
    "\n",
    "- **direction**: the optimization direction: \"minimize\" or \"maximize\". (Default: \"maximize\")\n",
    "\n",
    "- **eval_function**: user-defined evaluation function. **It must be a callable object, and be compatable with the `direction`**. (Default: None)\n",
    "\n",
    "    ```python\n",
    "    >>> from sklearn.metrics import cohen_kappa_score\n",
    "    >>> ……\n",
    "    >>> mymodel.optimize(\n",
    "    >>>     ……,\n",
    "    >>>     direction = \"maximize\",\n",
    "    >>>     eval_function = cohen_kappa_score,\n",
    "    >>>     ……\n",
    "    >>> )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.826658: 100%|██████████| 10/10 [00:28<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "mymodel.optimize(\n",
    "    strategy = \"tpe\",\n",
    "    cv = 5,\n",
    "    trials = 10,\n",
    "    n_jobs = 5,\n",
    "    direction = \"maximize\",\n",
    "    eval_function = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the model's accuracy\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **show_train**: Whether to show the accuracy on training set. (Default is `False`)\n",
    "\n",
    "- **dummy**: Whether to use a dummy estimator for comparison. (Default is `False`)\n",
    "\n",
    "- **eval_metric**: An user-defined evaluate metric. (Default is `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\mymodels\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\miniconda\\envs\\mymodels\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": {\n",
      "        \"test\": {\n",
      "            \"Overall Accuracy\": 0.8208955223880597,\n",
      "            \"Precision\": 0.8202415294475999,\n",
      "            \"Recall\": 0.8208955223880597,\n",
      "            \"F1\": 0.8205208492234899,\n",
      "            \"Kappa\": 0.6155868993545301,\n",
      "            \"Matthews Correlation Coefficient\": 0.6156658792988534,\n",
      "            \"Specificity\": 0.8630952380952381\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"Overall Accuracy\": 0.898876404494382,\n",
      "            \"Precision\": 0.8985556501894498,\n",
      "            \"Recall\": 0.898876404494382,\n",
      "            \"F1\": 0.8984200568507754,\n",
      "            \"Kappa\": 0.7853779904306221,\n",
      "            \"Matthews Correlation Coefficient\": 0.7859300160237404,\n",
      "            \"Specificity\": 0.931758530183727\n",
      "        }\n",
      "    },\n",
      "    \"dummy\": {\n",
      "        \"test\": {\n",
      "            \"Overall Accuracy\": 0.6268656716417911,\n",
      "            \"Precision\": 0.3929605702829138,\n",
      "            \"Recall\": 0.6268656716417911,\n",
      "            \"F1\": 0.4830891414487197,\n",
      "            \"Kappa\": 0.0,\n",
      "            \"Matthews Correlation Coefficient\": 0.0,\n",
      "            \"Specificity\": 1.0\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"Overall Accuracy\": 0.6115569823434992,\n",
      "            \"Precision\": 0.374001942653087,\n",
      "            \"Recall\": 0.6115569823434992,\n",
      "            \"F1\": 0.4641498212607036,\n",
      "            \"Kappa\": 0.0,\n",
      "            \"Matthews Correlation Coefficient\": 0.0,\n",
      "            \"Specificity\": 1.0\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mymodel.evaluate(\n",
    "    show_train = True,\n",
    "    dummy = True,\n",
    "    eval_metric = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explaining\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **select_background_data**: The data used for **background value calculation**. (Default is `\"train\"`)\n",
    "\n",
    "    Default is `\"train\"`, meaning that all data in the training set will be used. `\"test\"` means that all data in the test set will be used. `\"all\"` means that all data in the training and test set will be used. \n",
    "\n",
    "- **select_shap_data**: The data used for **calculating SHAP values**. Default is `\"test\"`, meaning that all data in the test set will be used. `\"all\"` means that all data in the training and test set will be used. (Default is `\"test\"`)\n",
    "\n",
    "- **sample_background_data_k**: Sampling the samples in the training set for **background value calculation**. (Default is `None`)\n",
    "    \n",
    "    Default `None`, meaning that all data in the training set will be used. An integer value means an actual number of data, while a float (i.e., 0.5) means the proportion in the training set for it. \n",
    "\n",
    "- **sample_shap_data_k**: Similar meaning to the `sample_background_data_k`. The test set will be implemented for **SHAP value calculation**. (Default is `None`)\n",
    "\n",
    "> SHAP currently doesn't support multi-class classification tasks when using **GBDT** models. This limitation may affect the interpretability results and users should verify compatibility with their use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.explain(\n",
    "    select_background_data = \"train\",\n",
    "    select_shap_data = \"test\",\n",
    "    sample_background_data_k = 50,\n",
    "    sample_shap_data_k = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction\n",
    "\n",
    "Finally, the optimized model and data engineering pipeline can be utilized to generate predictions on the test dataset.  \n",
    "\n",
    "In this step, the final predictions were saved in a file named `prediction.csv`, which includes two columns: one for the index and another for the predicted values.  \n",
    "\n",
    "The `prediction.csv` file can then be uploaded to the Kaggle platform to obtain a score for the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = pd.read_csv(\"data/titanic_test.csv\", encoding = \"utf-8\",\n",
    "                        na_values = np.nan, index_col = [\"PassengerId\"])\n",
    "\n",
    "data_pred = data_pred.loc[:, [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]]\n",
    "\n",
    "y_pred = mymodel.predict(data = data_pred)\n",
    "y_pred.name = \"Survived\"\n",
    "y_pred.to_csv(\"results/titanic/prediction.csv\", encoding = \"utf-8\", index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
